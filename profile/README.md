<!--

**Here are some ideas to get you started:**

ðŸ™‹â€â™€ï¸ A short introduction - what is your organization all about?
ðŸŒˆ Contribution guidelines - how can the community get involved?
ðŸ‘©â€ðŸ’» Useful resources - where can the community find your docs? Is there anything else the community should know?
ðŸ¿ Fun facts - what does your team eat for breakfast?
ðŸ§™ Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
<div align="center">
  <a href="https://lexsi.ai/">
    <img src="https://raw.githubusercontent.com/Lexsi-Labs/TabTune/refs/heads/docs/assets/lexsilogowhite.png" width="600">
  </a>
  <br>
  <a href="https://lexsi.ai/">https://www.lexsi.ai</a>
  <br><br>
  Paris ðŸ‡«ðŸ‡· Â· Mumbai ðŸ‡®ðŸ‡³ Â· London ðŸ‡¬ðŸ‡§ 
  <br><br>
  <a href="https://discord.gg/dSB62Q7A" style="display:inline-block; vertical-align:middle;">
    <img src="https://raw.githubusercontent.com/Lexsi-Labs/TabTune/refs/heads/docs/assets/discord.png" width="150">
  </a>
</div>

Lexsi Labs drives Aligned and Safe AI Frontier Research. Our goal is to build AI systems that are transparent, reliable, and value-aligned, combining interpretability, alignment, and governance to enable trustworthy intelligence at scale.  


### Research Focus  
- **Aligned & Safe AI:** Frameworks for self-monitoring, interpretable, and alignment-aware systems.  
- **Explainability & Alignment:** Faithful, architecture-agnostic interpretability and value-aligned optimization across tabular, vision, and language models.  
- **Safe Behaviour Control:** Techniques for fine-tuning, pruning, and behavioural steering in large models.  
- **Risk & Governance:** Continuous monitoring, drift detection, and fairness auditing for responsible deployment.  
- **Tabular & LLM Research:** Foundational work on tabular intelligence, in-context learning, and interpretable large language models.  


